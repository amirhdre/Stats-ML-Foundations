{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "904df11f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fitting a Line to Data: Least Squares and Linear Regression\n",
    "\n",
    "Linear regression is a fundamental technique in data analysis, used to fit a line that best describes the relationship between independent and dependent variables. This notebook explores the concepts, mathematics, and Python implementation of linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1d6f63",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Understanding the Concepts\n",
    "\n",
    "When fitting a line to data, our goal is to find the 'best' line that represents the trend in the data. One way to evaluate how well a line fits the data is by calculating the **residuals**, which are the differences between the observed data points and the predicted values on the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cefcce9e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53bf9537",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "x = np.linspace(0, 10, 20)\n",
    "y = 2 * x + 1 + np.random.normal(scale=2, size=x.shape)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x, y, color='blue', label='Data Points')\n",
    "plt.title('Scatter Plot of Data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef27332",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Residuals and the Line of Best Fit\n",
    "\n",
    "The **line of best fit** minimizes the sum of squared residuals:\n",
    "\n",
    "\\[ SSR = \\sum_{i=1}^{n}(y_i - (ax_i + b))^2 \\]\n",
    "\n",
    "Let's compute and visualize the residuals for a guessed line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4eaa1ede",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Guess a line and calculate residuals\n",
    "a, b = 2, 0  # Slope and intercept of the guessed line\n",
    "y_guess = a * x + b\n",
    "residuals = y - y_guess\n",
    "\n",
    "# Plot data, guessed line, and residuals\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x, y, color='blue', label='Data Points')\n",
    "plt.plot(x, y_guess, color='red', label='Guessed Line')\n",
    "for i in range(len(x)):\n",
    "    plt.plot([x[i], x[i]], [y[i], y_guess[i]], color='gray', linestyle='--')\n",
    "plt.title('Residuals and Guessed Line')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b3239",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Using Least Squares to Find the Best Fit\n",
    "\n",
    "Using the **method of least squares**, we find the line that minimizes the sum of squared residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bed02c4c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Perform linear regression using numpy.polyfit\n",
    "coefficients = np.polyfit(x, y, 1)\n",
    "best_fit_line = coefficients[0] * x + coefficients[1]\n",
    "\n",
    "# Plot the best-fit line\n",
    "# plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x, y, color='blue', label='Data Points')\n",
    "plt.plot(x, best_fit_line, color='green', label=f'Best Fit Line (y = {coefficients[0]:.2f}x + {coefficients[1]:.2f})')\n",
    "plt.title('Best Fit Line Using Least Squares')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2b98d3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Comparing Models\n",
    "\n",
    "Let's compare linear regression with a quadratic fit to observe the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed976c19",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.collections.PathCollection at 0x154a28a00>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<matplotlib.collections.PathCollection at 0x154a3a280>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<matplotlib.collections.PathCollection at 0x154a3aac0>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<matplotlib.collections.PathCollection at 0x154d38370>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a quadratic line\n",
    "# quadratic_coefficients = np.polyfit(x, y, 20)\n",
    "# quadratic_fit_line = np.polyval(quadratic_coefficients, x)\n",
    "y = 2 * x + 1 + np.random.normal(scale=2, size=x.shape)\n",
    "# # Perform linear regression using numpy.polyfit\n",
    "# coefficients = np\n",
    "# .polyfit(x, y, 1)\n",
    "# best_fit_line = coefficients[0] * x + coefficients[1]\n",
    "\n",
    "\n",
    "# Plot linear and quadratic fits\n",
    "# plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x, y, label='Data Points')\n",
    "# plt.plot(x, best_fit_line, color='green', label='Linear Fit')\n",
    "# plt.plot(x, quadratic_fit_line, color='purple', label='Quadratic Fit')\n",
    "# plt.title('Linear vs\n",
    "# . Quadratic Fit')\n",
    "# plt.xlabel('x')\n",
    "# plt.ylabel('y')\n",
    "# plt.legend()\n",
    "# plt.grid(alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Linear regression using least squares is a foundational method for fitting data. While linear fits are often sufficient, higher-order polynomials can better capture non-linear trends, though they risk overfitting. Always choose the simplest model that adequately explains your data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}