{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["### Neural Networks: Inside the Black Box\n", "# A Jupyter Notebook for Understanding Neural Networks\n\n", "This notebook provides an introduction to neural networks based on concepts from the video \"Neural Networks Pt. 1: Inside the Black Box\" and enhanced with detailed explanations and visualizations."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ## Import Required Libraries\n", "import numpy as np\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# ## What is a Neural Network?\n", "A neural network is a computational model inspired by the human brain. It consists of layers of interconnected nodes (neurons) that process data to identify patterns and make predictions. Neural networks are widely used in tasks such as image recognition, natural language processing, and complex regression problems."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# ## Dataset Simulation: Dosage Effectiveness\n", "Let's simulate the dataset used in the video, representing the effectiveness of a drug at different dosage levels."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Dosage levels\n", "x_dosage = np.array([0, 0.5, 1])\n", "# Effectiveness levels\n", "y_effectiveness = np.array([0, 1, 0])\n\n", "plt.scatter(x_dosage, y_effectiveness, color='blue', label='Data Points')\n", "plt.title(\"Dosage Effectiveness\")\n", "plt.xlabel(\"Dosage\")\n", "plt.ylabel(\"Effectiveness\")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# ## Components of a Neural Network\n", "Neural networks consist of the following key components:\n", "1. **Input Nodes**: Represent the input features (e.g., dosage levels).\n", "2. **Hidden Layers**: Contain nodes that transform inputs through activation functions to capture complex patterns.\n", "3. **Output Nodes**: Represent predictions or classifications (e.g., effectiveness).\n", "4. **Weights and Biases**: Parameters adjusted during training to minimize prediction errors.\n", "5. **Activation Functions**: Introduce non-linearity to enable modeling of complex relationships."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# ## Activation Functions\n", "Activation functions are mathematical functions applied to the output of a neuron to introduce non-linearity. They are essential for neural networks to model complex, non-linear data patterns."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ### Common Activation Functions\n", "# - **SoftPlus**: \\( \\text{SoftPlus}(x) = \\log(1 + e^x) \\) (smooth approximation of ReLU).\n", "# - **ReLU**: \\( \\text{ReLU}(x) = \\max(0, x) \\) (rectified linear unit, commonly used).\n", "# - **Sigmoid**: \\( \\text{Sigmoid}(x) = \\frac{1}{1 + e^{-x}} \\) (used for probabilistic outputs).\n\n", "def softplus(x):\n", "    return np.log1p(np.exp(x))\n\n", "def relu(x):\n", "    return np.maximum(0, x)\n\n", "def sigmoid(x):\n", "    return 1 / (1 + np.exp(-x))\n\n", "# Visualize activation functions\n", "x = np.linspace(-10, 10, 100)\n", "plt.plot(x, softplus(x), label=\"SoftPlus\")\n", "plt.plot(x, relu(x), label=\"ReLU\")\n", "plt.plot(x, sigmoid(x), label=\"Sigmoid\")\n", "plt.title(\"Activation Functions\")\n", "plt.xlabel(\"Input\")\n", "plt.ylabel(\"Output\")\n", "plt.legend()\n", "plt.grid()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# ## Building a Simple Neural Network\n", "### Neural Network Architecture\n", "- **Input Layer**: 1 node (dosage input).\n", "- **Hidden Layer**: 2 nodes (SoftPlus activation).\n", "- **Output Layer**: 1 node (predicted effectiveness)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Mathematical Representation\n", "The network transforms inputs as follows:\n", "1. Input values are multiplied by weights and added to biases.\n", "2. The result is passed through an activation function.\n", "3. Outputs from the hidden layer are combined in the output layer using weights and biases."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define parameters\n", "weights_hidden = np.array([-34.4, -2.52])  # Weights for connections to hidden layer\n", "biases_hidden = np.array([2.14, 1.29])     # Biases for hidden layer\n", "weights_output = np.array([-1.3, 2.28])    # Weights for connections to output layer\n", "bias_output = -0.58                       # Bias for output layer\n\n", "def neural_network(x):\n", "    \"\"\"Simple neural network to compute predictions.\"\"\"\n", "    # Hidden layer computations\n", "    hidden_inputs = weights_hidden * x + biases_hidden\n", "    hidden_outputs = softplus(hidden_inputs)\n", "    # Output layer computation\n", "    output = np.dot(hidden_outputs, weights_output) + bias_output\n", "    return output"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# ## Generating the Green Squiggle"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_values = np.linspace(0, 1, 100)\n", "y_values = [neural_network(x) for x in x_values]\n\n", "plt.plot(x_values, y_values, label=\"Green Squiggle\", color='green')\n", "plt.scatter(x_dosage, y_effectiveness, color='blue', label=\"Data Points\")\n", "plt.title(\"Neural Network Prediction\")\n", "plt.xlabel(\"Dosage\")\n", "plt.ylabel(\"Effectiveness\")\n", "plt.legend()\n", "plt.grid()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# ## Key Takeaways\n", "1. **Neural networks** use layers of interconnected nodes to model complex relationships in data.\n", "2. **Weights and biases** are adjusted during training to fit the data.\n", "3. **Activation functions** enable neural networks to learn non-linear patterns.\n", "4. Even a simple neural network with one hidden layer can effectively fit non-linear data.\n", "5. By adding more hidden layers and nodes, neural networks can model increasingly complex datasets."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# ## What Comes Next?\n", "In the next part of this series, we will explore **backpropagation**, the method used to train neural networks by optimizing weights and biases to minimize prediction errors. We will also discuss variations like deep learning and the impact of different architectures."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}}, "nbformat": 4, "nbformat_minor": 5}