{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17817ed2",
   "metadata": {},
   "source": [
    "\n",
    "# Neural Networks: Understanding ArgMax and SoftMax\n",
    "\n",
    "This notebook explores two essential components of neural networks: the **ArgMax** and **SoftMax** functions. These functions are critical for interpreting model outputs, training neural networks, and connecting predicted values with probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. ArgMax\n",
    "\n",
    "### Definition\n",
    "The **ArgMax** function identifies the index of the maximum value in a set of outputs. For example, if the raw output of a neural network is:\n",
    "\n",
    "$$ [1.43, -0.4, 0.23] $$\n",
    "\n",
    "The **ArgMax** function selects the index of the largest value, which corresponds to $1.43$.\n",
    "\n",
    "### Mathematical Representation\n",
    "Given a vector $z = [z_1, z_2, ..., z_n]$, ArgMax is defined as:\n",
    "\n",
    "$$ \\text{ArgMax}(z) = \\text{argmax}_i(z_i), $$\n",
    "\n",
    "where $i$ is the index of the maximum value in $z$.\n",
    "\n",
    "### Example\n",
    "For $z = [1.43, -0.4, 0.23]$, ArgMax assigns:\n",
    "\n",
    "$$ [1, 0, 0]. $$\n",
    "\n",
    "### Limitations of ArgMax\n",
    "- **Non-Differentiable:** The derivative of ArgMax is undefined or constant, making it unsuitable for gradient-based optimization like backpropagation.\n",
    "- **No Probabilistic Interpretation:** ArgMax provides a single prediction without expressing confidence or probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. SoftMax\n",
    "\n",
    "### Definition\n",
    "The **SoftMax** function transforms raw neural network outputs (logits) into probabilities. It ensures the outputs are between $0$ and $1$ and sum to $1$, making them interpretable as probabilities.\n",
    "\n",
    "### Mathematical Representation\n",
    "Given raw outputs $z = [z_1, z_2, ..., z_n]$, the SoftMax function is defined as:\n",
    "\n",
    "$$ \\text{SoftMax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}. $$\n",
    "\n",
    "### Properties of SoftMax\n",
    "1. **Probabilities:** Output values are in $[0, 1]$ and sum to $1$.\n",
    "2. **Ranking Preserved:** The largest input maps to the largest probability.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why SoftMax is Preferred for Training\n",
    "\n",
    "### Derivative of SoftMax\n",
    "For a given class $i$, the derivative with respect to the raw output $z_i$ is:\n",
    "\n",
    "$$ \\frac{\\partial \\text{SoftMax}(z_i)}{\\partial z_i} = \\text{SoftMax}(z_i) \\cdot (1 - \\text{SoftMax}(z_i)). $$\n",
    "\n",
    "For $i \\neq j$:\n",
    "\n",
    "$$ \\frac{\\partial \\text{SoftMax}(z_i)}{\\partial z_j} = - \\text{SoftMax}(z_i) \\cdot \\text{SoftMax}(z_j). $$\n",
    "\n",
    "### Example\n",
    "Using $z = [1.43, -0.4, 0.23]$:\n",
    "\n",
    "1. Compute $e^{z_i}$ for each value:\n",
    "   $$ e^{1.43} \\approx 4.18, \\, e^{-0.4} \\approx 0.67, \\, e^{0.23} \\approx 1.26. $$\n",
    "2. Compute the sum: $4.18 + 0.67 + 1.26 = 6.11$.\n",
    "3. Compute probabilities:\n",
    "   $$ \\text{SoftMax}(1.43) = \\frac{4.18}{6.11} \\approx 0.69, \\text{SoftMax}(-0.4) \\approx 0.1, \\text{SoftMax}(0.23) \\approx 0.21. $$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Combining ArgMax and SoftMax\n",
    "\n",
    "- **Training:** Use SoftMax for gradient-based optimization.\n",
    "- **Inference:** Use ArgMax to select the most likely class.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Practical Implementation\n",
    "\n",
    "Below, we demonstrate ArgMax and SoftMax using Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d31f5e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMax Outputs: [0.68417808 0.10975145 0.20607048]\n",
      "ArgMax Output (Index): 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define raw outputs (logits)\n",
    "logits = np.array([1.43, -0.4, 0.23])\n",
    "\n",
    "# SoftMax Function\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits)\n",
    "    return exp_logits / np.sum(exp_logits)\n",
    "\n",
    "# ArgMax Function\n",
    "def argmax(logits):\n",
    "    return np.argmax(logits)\n",
    "\n",
    "# Compute SoftMax and ArgMax\n",
    "softmax_outputs = softmax(logits)\n",
    "argmax_output = argmax(logits)\n",
    "\n",
    "print(\"SoftMax Outputs:\", softmax_outputs)\n",
    "print(\"ArgMax Output (Index):\", argmax_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7754d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
