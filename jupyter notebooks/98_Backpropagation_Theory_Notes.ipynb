{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973c0e68",
   "metadata": {},
   "source": [
    "# Theory Notes: Backpropagation and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73f9fd",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "Backpropagation is a method used to train neural networks. It calculates the gradient of the loss function with respect to each parameter (weights and biases) using the chain rule, then updates the parameters using gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfe4e18",
   "metadata": {},
   "source": [
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Forward Pass**: Compute the predictions of the network:\n",
    "   $$\n",
    "   \\hat{y} = f(X; W, b)\n",
    "   $$\n",
    "\n",
    "2. **Loss Function**: Quantifies the difference between predictions and actual values. For Mean Squared Error (MSE):\n",
    "   $$\n",
    "   L = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "   $$\n",
    "\n",
    "3. **Gradient Descent**: Update parameters to minimize the loss:\n",
    "   $$\n",
    "   \theta \\leftarrow \theta - \u0007lpha \\frac{\\partial L}{\\partial \theta}\n",
    "   $$\n",
    "   where $\\alpha$ is the learning rate.\n",
    "\n",
    "4. **Chain Rule**: Used to compute gradients in multi-layer networks:\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial \theta} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial \theta}\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cda0dfa",
   "metadata": {},
   "source": [
    "\n",
    "## Backpropagation Steps\n",
    "\n",
    "1. **Compute the loss**: \n",
    "   $$\n",
    "   L = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "   $$\n",
    "\n",
    "2. **Backward Pass**:\n",
    "   - Calculate $\\frac{\\partial L}{\\partial \\hat{y}}$:\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial \\hat{y}} = -2(y - \\hat{y})\n",
    "     $$\n",
    "   - Use the chain rule to compute $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial b}$.\n",
    "\n",
    "3. **Parameter Update**:\n",
    "   $$\n",
    "   W \\leftarrow W - \\alpha \\frac{\\partial L}{\\partial W}, \\quad b \\leftarrow b - \\alpha \\frac{\\partial L}{\\partial b}\n",
    "   $$\n",
    "\n",
    "Repeat these steps until the loss converges or the maximum number of iterations is reached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2347852b",
   "metadata": {},
   "source": [
    "\n",
    "## Neural Network Layers\n",
    "\n",
    "1. **Input Layer**: Receives input features $X$.\n",
    "2. **Hidden Layers**: Transform inputs using weights $W$ and biases $b$.\n",
    "   $$\n",
    "   h = \\sigma(XW + b)\n",
    "   $$\n",
    "   where $\\sigma$ is an activation function (e.g., Sigmoid, ReLU).\n",
    "\n",
    "3. **Output Layer**: Produces the prediction:\n",
    "   $$\n",
    "   \\hat{y} = \\sigma(hW_o + b_o)\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eab298",
   "metadata": {},
   "source": [
    "\n",
    "## Chain Rule in Depth\n",
    "\n",
    "For a multi-layer network:\n",
    "- The gradient of the loss with respect to weights in layer $k$ is:\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial W_k} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial W_k}\n",
    "  $$\n",
    "\n",
    "- Gradients flow backward from the output layer to the input layer, updating parameters layer by layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8312b4",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Backpropagation enables neural networks to learn by iteratively updating parameters to minimize the loss. It combines:\n",
    "- The chain rule for gradient computation.\n",
    "- Gradient descent for optimization.\n",
    "\n",
    "This theoretical foundation is crucial for understanding deep learning.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
