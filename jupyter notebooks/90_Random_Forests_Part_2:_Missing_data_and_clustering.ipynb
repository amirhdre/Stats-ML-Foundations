{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random Forests Part 2: Missing data and clustering\n",
        "\n",
        "In this notebook, we will focus on handling missing data and sample clustering in random forests. These are essential aspects of creating robust and accurate random forest models. We will use a patient dataset, where some patients have missing information for certain features.\n",
        "\n",
        "We will consider two types of missing data:\n",
        "1. Missing data in the original dataset used to create the random forest.\n",
        "2. Missing data in a new sample that we want to categorize.\n",
        "\n",
        "Let's start by importing the necessary Python libraries and loading the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "import seaborn as sns\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('patients_data.csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handling missing data in the original dataset\n",
        "\n",
        "The general idea for dealing with missing data in the original dataset is to make an initial guess (which could be bad) and then gradually refine the guess until it is hopefully a good guess. We will use the most common value or the median value for categorical and numerical features respectively as our initial guess.\n",
        "\n",
        "Let's implement this approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill in missing values\n",
        "for column in data.columns:\n",
        "    if data[column].dtype == 'object':\n",
        "        data[column].fillna(data[column].mode()[0], inplace=True)\n",
        "    else:\n",
        "        data[column].fillna(data[column].median(), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Determining similarity\n",
        "\n",
        "Now, we want to refine these initial guesses by determining which samples are similar to the ones with missing data. The steps are:\n",
        "1. Build a random forest with the data.\n",
        "2. Run all of the data down all of the trees.\n",
        "3. Keep track of similar samples using a proximity matrix.\n",
        "\n",
        "We define two samples as similar if they end up in the same leaf node in a tree. Let's calculate the proximity matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_proximity_matrix(model, X):\n",
        "    \"\"\"\n",
        "    Calculate the proximity matrix based on the decision path of a random forest model.\n",
        "    \"\"\"\n",
        "    leaves = model.apply(X)\n",
        "    n_samples = leaves.shape[0]\n",
        "    proximity_matrix = np.zeros((n_samples, n_samples))\n",
        "    for sample_i in range(n_samples):\n",
        "        for sample_j in range(sample_i, n_samples):\n",
        "            proximity_matrix[sample_i, sample_j] = (leaves[sample_i] == leaves[sample_j]).mean()\n",
        "            proximity_matrix[sample_j, sample_i] = proximity_matrix[sample_i, sample_j]\n",
        "    return proximity_matrix\n",
        "\n",
        "# Fit a random forest model\n",
        "model = RandomForestClassifier(random_state=0)\n",
        "model.fit(data.iloc[:, :-1], data.iloc[:, -1])\n",
        "\n",
        "# Calculate proximity matrix\n",
        "proximity_matrix = calculate_proximity_matrix(model, data.iloc[:, :-1])\n",
        "\n",
        "# Plot proximity matrix\n",
        "sns.heatmap(proximity_matrix, cmap='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Refining the guess\n",
        "\n",
        "Now we use the proximity values to make better guesses about the missing data. For categorical data, we calculate a weighted frequency of each category using proximity values as the weights. For numerical data, we use the proximities to calculate a weighted average.\n",
        "\n",
        "Let's implement this approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def refine_guesses(data, proximity_matrix):\n",
        "    \"\"\"\n",
        "    Refine the guesses of missing values in data based on the proximity matrix.\n",
        "    \"\"\"\n",
        "    refined_data = data.copy()\n",
        "    for column in data.columns:\n",
        "        if data[column].isnull().sum() > 0:\n",
        "            for i in data[data[column].isnull()].index:\n",
        "                if data[column].dtype == 'object':\n",
        "                    # Calculate weighted frequency for categorical data\n",
        "                    weighted_freq = data.loc[:, column].value_counts() / data.loc[:, column].value_counts().sum()\n",
        "                    for category in weighted_freq.index:\n",
        "                        weighted_freq[category] *= proximity_matrix[i, data.loc[:, column] == category].sum()\n",
        "                    weighted_freq /= weighted_freq.sum()\n",
        "                    # Assign the category with the highest weighted frequency\n",
        "                    refined_data.loc[i, column] = weighted_freq.idxmax()\n",
        "                else:\n",
        "                    # Calculate weighted average for numerical data\n",
        "                    weights = proximity_matrix[i, ~data.loc[:, column].isnull()]\n",
        "                    values = data.loc[~data.loc[:, column].isnull(), column]\n",
        "                    refined_data.loc[i, column] = np.average(values, weights=weights)\n",
        "    return refined_data\n",
        "\n",
        "# Refine the initial guesses\n",
        "refined_data = refine_guesses(data, proximity_matrix)\n",
        "refined_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handling missing data in a new sample\n",
        "\n",
        "Now consider the case where we have missing data in a new sample that we want to categorize. We will create two copies of the data, one that has the target class and one that doesn't. Then we use the iterative method we talked about to make a good guess about the missing values. Let's implement this approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_missing_new_sample(model, sample, data, proximity_matrix):\n",
        "    \"\"\"\n",
        "    Handle missing values in a new sample for categorization based on a random forest model.\n",
        "    \"\"\"\n",
        "    # Create two copies of the new sample\n",
        "    sample_yes = sample.copy()\n",
        "    sample_no = sample.copy()\n",
        "    sample_yes['target'] = 'yes'\n",
        "    sample_no['target'] = 'no'\n",
        "    \n",
        "    # Use the iterative method to make a good guess about the missing values\n",
        "    for _ in range(10):\n",
        "        proximity_matrix_yes = calculate_proximity_matrix(model, pd.concat([data, sample_yes], ignore_index=True))\n",
        "        proximity_matrix_no = calculate_proximity_matrix(model, pd.concat([data, sample_no], ignore_index=True))\n",
        "        sample_yes = refine_guesses(sample_yes, proximity_matrix_yes[-1, :-1])\n",
        "        sample_no = refine_guesses(sample_no, proximity_matrix_no[-1, :-1])\n",
        "    \n",
        "    # Run the two samples down the trees in the forest and see which of the two is correctly labeled by the random forest the most times\n",
        "    prediction_yes = model.predict_proba(sample_yes.iloc[:, :-1])\n",
        "    prediction_no = model.predict_proba(sample_no.iloc[:, :-1])\n",
        "    \n",
        "    return 'yes' if prediction_yes[0, 1] > prediction_no[0, 1] else 'no'\n",
        "\n",
        "# Missing data in a new sample\n",
        "new_sample = pd.DataFrame([[np.nan, 180, 'yes']], columns=data.columns[:-1])\n",
        "target = handle_missing_new_sample(model, new_sample, data, proximity_matrix)\n",
        "print(f'The predicted target for the new sample is: {target}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's it! Now you have learned how to handle missing data and perform sample clustering in random forests. This knowledge will help you create more robust and accurate random forest models. Happy machine learning!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
