{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# t-SNE, Clearly Explained\n",
        "\n",
        "t-SNE (t-Distributed Stochastic Neighbor Embedding) is a technique used for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. It is extensively applied in image processing, NLP, genomic data and speech processing.\n",
        "\n",
        "To understand t-SNE thoroughly, we will start from the basics of dimensionality reduction, then we will move on to the working of t-SNE and finally we will see a working example of t-SNE with the help of `scikit-learn`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dimensionality Reduction\n",
        "\n",
        "Dimensionality reduction is the process of reducing the number of random variables under consideration, via obtaining a set of principal variables. It can be divided into feature selection and feature extraction.\n",
        "\n",
        "High-dimensional data can be almost impossible to visualize, but reducing the dimensionality to 2 or 3 can allow us to plot the data in a way that we can understand visually.\n",
        "\n",
        "Dimensionality reduction can be linear (like PCA) or non-linear (like t-SNE, LLE etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## t-SNE: Overview\n",
        "\n",
        "The t-SNE algorithm comprises two main stages. \n",
        "1. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a lower probability.\n",
        "2. In the second step, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback-Leibler (KL) divergence between the two distributions with respect to the locations of the points in the map.\n",
        "\n",
        "While the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this should be changed as appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load the digits dataset which is available in `sklearn.datasets`. This data consists of 8x8 pixel images, meaning they are 64-dimensional. We will use t-SNE to visualize these digits in 2 dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "digits = load_digits()\n",
        "print(digits.data.shape)\n",
        "plt.gray()\n",
        "plt.matshow(digits.images[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we will use `TSNE` class from `sklearn.manifold` module to reduce the dimensionality of our digits data from 64 to 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "\n",
        "X_2d = tsne.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our two-dimensional representation, we can plot it using matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_ids = range(len(digits.target_names))\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "colors = 'r', 'g', 'b', 'c', 'm', 'y', 'k', 'w', 'orange', 'purple'\n",
        "for i, c, label in zip(target_ids, colors, digits.target_names):\n",
        "    plt.scatter(X_2d[y == i, 0], X_2d[y == i, 1], c=c, label=label)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, t-SNE algorithm was able to take the 64-dimensional digits data and represent it in a two-dimensional graph, while maintaining relative distances, therefore preserving clusters of digits that are similar to each other."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
