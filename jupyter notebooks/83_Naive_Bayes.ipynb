Here is the JSON representation of a Jupyter notebook that explains the Naive Bayes algorithm based on the video:

```json
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "Naive Bayes is a simple but surprisingly powerful algorithm for predictive modeling. The model is comprised of two types of probabilities that can be calculated directly from your training data: the probability of each class and the conditional probability for each class given each x value. Once calculated, the probability model can be used to make predictions for new data using Bayes Theorem. When your data is real-valued it is common to assume that the data distribution is Gaussian or Normal.\n",
    "\n",
    "In this notebook, we'll explain the Naive Bayes classifier using a simple example. This explanation is based on this YouTube video by StatQuest: [Naive Bayes, Clearly Explained](https://www.youtube.com/watch?v=O2L2Uv9pdDA).\n",
    "\n",
    "![Naive_Bayes](https://miro.medium.com/max/1200/1*sOtXk9XlKCQ6Y3FJJ4w8Ew.jpeg)\n",
    "\n",
    "## Naive Bayes Classifier\n",
    "The naive Bayes classifier is based on Bayes’ theorem with the assumption of independence between every pair of features. Naive Bayes classifiers work well in many real-world situations such as document classification and spam filtering.\n",
    "\n",
    "Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem.\n",
    "\n",
    "### Bayes' Theorem\n",
    "Bayes’ theorem describes the relationship of conditional probabilities of statistical quantities. In Bayesian classification, we’re interested in finding the probability of a label given some observed features, which we can write as $P(L | features)$. Bayes’ theorem tells us how to express this in terms of quantities we can compute more directly:\n",
    "\n",
    "$$P(L | features) = \\frac{P(features | L) * P(L)}{P(features)}$$\n",
    "\n",
    "If we are trying to decide between two labels—let’s call them $L1$ and $L2$—then one way to make this decision is to compute the ratio of the posterior probabilities for each label:\n",
    "\n",
    "$$\\frac{P(L1 | features)}{P(L2 | features)} = \\frac{P(features | L1) * P(L1)}{P(features | L2) * P(L2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with Example\n",
    "Let's use the spam email classification example from the video to explain the naive Bayes step by step.\n",
    "\n",
    "We have received some emails, some of them are normal (ham) and some of them are spam. We want to build a naive Bayes classifier to classify the emails.\n",
    "\n",
    "Here is the data: \n",
    "\n",
    "* Normal emails: \n",
    "    * 'Dear, friend'\n",
    "    * 'Lunch, money'\n",
    "    * 'Dear, friend'\n",
    "    * 'Dear, friend'\n",
    "    * 'Dear, friend'\n",
    "    * 'Lunch, money'\n",
    "    * 'Lunch, money'\n",
    "    * 'Lunch, money'\n",
    "* Spam emails: \n",
    "    * 'Dear, friend, money'\n",
    "    * 'Money, money, money'\n",
    "    * 'Money, money, money'\n",
    "    * 'Money, money, money'\n",
    "\n",
    "We can represent the data as a frequency histogram for each word. But in practice, we calculate the probability of each word in the normal emails and spam emails respectively.\n",
    "\n",
    "### Calculate the Probabilities\n",
    "We use the following formula to calculate the probability of each word in the normal emails and spam emails:\n",
    "\n",
    "$$P(word | class) = \\frac{Occurrences of the word in the class}{Total words in the class}$$\n",
    "\n",
    "We also need to calculate the prior probability of each class (normal and spam email). We use the following formula to calculate the prior probability:\n",
    "\n",
    "$$P(class) = \\frac{Number of emails in the class}{Total number of emails}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# The data\n",
    "normal_emails = ['dear friend', 'lunch money', 'dear friend', 'dear friend', 'dear friend', 'lunch money', 'lunch money', 'lunch money']\n",
    "spam_emails = ['dear friend money', 'money money money', 'money money money', 'money money money']\n",
    "\n",
    "# Calculate the total number of words in normal emails and spam emails\n",
    "total_normal_words = sum([len(email.split(' ')) for email in normal_emails])\n",
    "total_spam_words = sum([len(email.split(' ')) for email in spam_emails])\n",
    "\n",
    "# Calculate the occurrences of each word in normal emails and spam emails\n",
    "normal_counter = Counter(' '.join(normal_emails).split(' '))\n",
    "spam_counter = Counter(' '.join(spam_emails).split(' '))\n",
    "\n",
    "# Calculate the probabilities of each word in normal emails and spam emails\n",
    "normal_prob = {word: count/total_normal_words for word, count in normal_counter.items()}\n",
    "spam_prob = {word: count/total_spam_words for word, count in spam_counter.items()}\n",
    "\n",
    "# Calculate the prior probabilities\n",
    "prior_normal = len(normal_emails) / (len(normal_emails) + len(spam_emails))\n",
    "prior_spam = len(spam_emails) / (len(normal_emails) + len(spam_emails))\n",
    "\n",
    "# Print the probabilities\n",
    "print('Prior probabilities:')\n",
    "print('P(normal) =', prior_normal)\n",
    "print('P(spam) =', prior_spam)\n",
    "print('\\n')\n",
    "print('Word probabilities in normal emails:')\n",
    "for word, prob in normal_prob.items():\n",
    "    print('P(' + word + '|normal) =', prob)\n",
    "print('\\n')\n",
    "print('Word probabilities in spam emails:')\n",
    "for word, prob in spam_prob.items():\n",
    "    print('P(' + word + '|spam) =', prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions\n",
    "Now that we have calculated the probabilities, we can make predictions for new emails. The prediction is based on the posterior probability of each class given the email's words. We use the following formula to calculate the posterior probability:\n",
    "\n",
    "$$P(class | words) \\propto P(class) * P(word_1 | class) * P(word_2 | class) * ... * P(word_n | class)$$\n",
    "\n",
    "This formula is derived from Bayes' theorem. We make the naive assumption that the words are independent of each other. This is why it's called 'naive' Bayes.\n",
    "\n",
    "Let's make a prediction for the email 'lunch money money money' using the probabilities we've calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(email, normal_prob, spam_prob, prior_normal, prior_spam):\n",
    "    words = email.split(' ')\n",
    "    # Calculate the posterior probabilities\n",
    "    post_normal = np.log(prior_normal) + sum(np.log(normal_prob.get(word, 1e-6)) for word in words)\n",
    "    post_spam = np.log(prior_spam) + sum(np.log(spam_prob.get(word, 1e-6)) for word in words)\n",
    "    # Make the prediction\n",
    "    if post_normal > post_spam:\n",
    "        return 'normal'\n",
    "    else:\n",
    "        return 'spam'\n",
    "\n",
    "# Make a prediction for the email 'lunch money money money'\n",
    "email = 'lunch money money money'\n",
    "prediction = predict(email, normal_prob, spam_prob, prior_normal, prior_spam)\n",
    "print('The email \\'{}\\' is predicted as {}.'.format(email, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "One potential issue with the naive Bayes classifier is that it cannot handle new words that did not occur in the training data. The probability of the new word given a class is 0, which will make the posterior probability 0 no matter what other words the email contains.\n",
    "\n",
    "A common solution to this problem is to use a technique called smoothing. In smoothing, we add a small constant to the counts of each word. This ensures that the probability of each word will never be 0. The most common type of smoothing used with naive Bayes is Laplace smoothing or add-1 smoothing.\n",
    "\n",
    "Let's update our probability calculation to include smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The smoothing constant\n",
    "alpha = 1\n",
    "\n",
    "# Update the total number of words\n",
    "total_normal_words += alpha * len(normal_counter)\n",
    "total_spam_words += alpha * len(spam_counter)\n",
    "\n",
    "# Update the occurrences of each word\n",
    "normal_counter = {word: count + alpha for word, count in normal_counter.items()}\n",
    "spam_counter = {word: count + alpha for word, count in spam_counter.items()}\n",
    "\n",
    "# Calculate the probabilities of each word in normal emails and spam emails\n",
    "normal_prob = {word: count/total_normal_words for word, count in normal_counter.items()}\n",
    "spam_prob = {word: count/total_spam_words for word, count in spam_counter.items()}\n",
    "\n",
    "# Print the updated probabilities\n",
    "print('Word probabilities in normal emails after smoothing:')\n",
    "for word, prob in normal_prob.items():\n",
    "    print('P(' + word + '|normal) =', prob)\n",
    "print('\\n')\n",
    "print('Word probabilities in spam emails after smoothing:')\n",
    "for word, prob in spam_prob.items():\n",
    "    print('P(' + word + '|spam) =', prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions with Smoothing\n",
    "Now that we've added smoothing, we can make predictions for emails with new words. Let's make a prediction for the email 'dear lunch' using the updated probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction for the email 'dear lunch'\n",
    "email = 'dear lunch'\n",
    "prediction = predict(email, normal_prob, spam_prob, prior_normal, prior_spam)\n",
    "print('The email \\'{}\\' is predicted as {}.'.format(email, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we've explained the naive Bayes classifier using a simple example. We've shown how to calculate the probabilities needed for the classifier, how to make predictions, and how to handle new words using smoothing. Despite its simplicity and the naive assumption of feature independence, the naive Bayes classifier often performs well in practice and is especially popular in text classification tasks such as spam filtering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor":