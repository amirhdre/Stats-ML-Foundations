{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information, Clearly Explained\n",
    "\n",
    "In this notebook, we will explain and implement the concept of Mutual Information. Mutual Information is a powerful tool used in machine learning and statistics to quantify the relationship between two random variables.\n",
    "\n",
    "We will start by explaining the concept of Mutual Information, go through the mathematical derivation, and then apply it to a simple dataset. After that, we will also discuss its applications and limitations.\n",
    "\n",
    "Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn import datasets\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Mutual Information?\n",
    "\n",
    "Mutual information is a measure that quantifies the amount of information obtained about one random variable through observing the other random variable. It is a measure of the mutual dependence between the two variables. More specifically, it quantifies the \"amount of information\" (in units such as bits) obtained about one random variable, through the other random variable.\n",
    "\n",
    "The concept of Mutual Information is closely related to that of entropy. While entropy measures the uncertainty of a single random variable, mutual information measures the reduction in uncertainty for one random variable given the knowledge of another.\n",
    "\n",
    "It is important to note that mutual information is symmetric, i.e., $I(X; Y) = I(Y; X)$.\n",
    "\n",
    "Mathematically, it is defined as:\n",
    "\n",
    "$$I(X; Y) = \\sum_{y \\in Y} \\sum_{x \\in X} p(x, y) \\log\\left(\\frac{p(x, y)}{p(x)p(y)}\\right)$$\n",
    "\n",
    "where:\n",
    "- $X$ and $Y$ are random variables,\n",
    "- $p(x, y)$ is the joint probability distribution function of $X$ and $Y$,\n",
    "- $p(x)$ and $p(y)$ are the marginal probability distribution functions of $X$ and $Y$ respectively.\n",
    "\n",
    "The equation can seem a little intimidating, but it can be broken down into smaller parts. Let's go through it step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Mutual Information Equation\n",
    "\n",
    "The mutual information equation contains several parts. Let's go through them one by one.\n",
    "\n",
    "- **Joint Probability $p(x, y)$:** This denotes the probability of $X$ and $Y$ occurring together. For example, if $X$ is the event of raining and $Y$ is the event of carrying an umbrella, $p(x, y)$ would denote the probability of it raining and carrying an umbrella at the same time.\n",
    "\n",
    "- **Marginal Probability $p(x)$ and $p(y)$:** These denote the probabilities of $X$ and $Y$ occurring separately. Using the same example as above, $p(x)$ would denote the probability of it raining and $p(y)$ would denote the probability of carrying an umbrella, without considering the other event.\n",
    "\n",
    "- **The Log Term:** The term inside the log is the ratio of the joint probability to the product of the marginal probabilities. This ratio tells us how much more (or less) often the events $x$ and $y$ occur together than we would expect if they were independent. The log of this ratio then gives us a measure of this 'extra' co-occurrence in terms of bits.\n",
    "\n",
    "The outer sum essentially tells us to compute this 'extra' co-occurrence information for each possible pair of outcomes $(x, y)$, and then add them up to get the total mutual information between $X$ and $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Mutual Information in Python\n",
    "\n",
    "Now that we understand what Mutual Information is, let's see how we can compute it in Python. We will use the mutual information functions provided by the `scikit-learn` library. Let's first start with a simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(1000, 3)\n",
    "Y = X[:, 0] + np.sin(6 * np.pi * X[:, 1]) + 0.1 * np.random.randn(1000)\n",
    "\n",
    "# Compute Mutual Information between each feature and the target\n",
    "mi = mutual_info_regression(X, Y)\n",
    "\n",
    "# Print the Mutual Information values\n",
    "for i, val in enumerate(mi):\n",
    "    print(f\"Mutual Information between Feature {i} and the target: {val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the mutual information between the first feature and the target is the highest, while the mutual information between the third feature and the target is the lowest. This is consistent with how we defined our target $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Mutual Information\n",
    "\n",
    "Mutual information has a wide range of applications in machine learning. Some of them include:\n",
    "\n",
    "- **Feature Selection:** Mutual Information can be used to measure the relevance of features in a dataset. Features with high mutual information with the target variable are considered relevant, while features with low mutual information are considered irrelevant and can be discarded.\n",
    "\n",
    "- **Model Evaluation:** Mutual Information can also be used to evaluate the performance of a model. A model that captures more mutual information between the input features and the target variable is considered better.\n",
    "\n",
    "- **Clustering:** Mutual Information can be used as a similarity measure for clustering. Two clusters that share more mutual information are considered more similar.\n",
    "\n",
    "## Limitations of Mutual Information\n",
    "\n",
    "Despite its usefulness, Mutual Information also has a few limitations:\n",
    "\n",
    "- **Estimation Difficulty:** The estimation of Mutual Information can be difficult, especially in high dimensions. The estimation can be sensitive to the number of bins in the histogram and the bandwidth of the kernel density estimation.\n",
    "\n",
    "- **Assumption of Dependence:** Mutual Information assumes that the variables are dependent. If the variables are independent, the mutual information will be zero, but the converse is not necessarily true.\n",
    "\n",
    "- **Lack of Directionality:** Mutual Information is symmetric and does not provide any information about the direction of the relationship between variables.\n",
    "\n",
    "Despite these limitations, Mutual Information remains a powerful tool for understanding and quantifying the relationships between variables in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [Elements of Information Theory - Thomas M. Cover, Joy A. Thomas](https://www.amazon.com/Elements-Information-Theory-Telecommunications-Processing/dp/0471241954)\n",
    "2. [Mutual Information - Wikipedia](https://en.wikipedia.org/wiki/Mutual_information)\n",
    "3. [StatQuest: Mutual Information, Clearly Explained](https://www.youtube.com/watch?v=r4sas27zNWQ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
