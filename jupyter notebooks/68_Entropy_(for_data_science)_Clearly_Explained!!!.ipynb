{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entropy for Data Science\n",
        "\n",
        "In this notebook, we will explore the concept of entropy, which is widely used in data science. It is used to build classification trees, it forms the basis of mutual information that quantifies the relationship between two things, and it's the basis of relative entropy (aka the Kullback-Leibler distance) and cross entropy. All these methods use entropy or something derived from it to quantify similarities and differences.\n",
        "\n",
        "Before we delve into entropy, we need to understand the idea of 'surprise'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Surprise\n",
        "\n",
        "The concept of surprise is best understood with an example. Suppose we have two types of chickens, orange and blue, organized into three separate areas A, B, and C. If we randomly pick up a chicken in area A, where there are six orange chickens and only one blue chicken, there is a higher probability that we will pick up an orange chicken. Thus, it would not be very surprising if we did.\n",
        "\n",
        "If we pick up the blue chicken from area A, we would be relatively surprised because the probability of picking up a blue chicken is low. The surprise is in some way inversely related to probability - when the probability is low, the surprise is high, and vice versa.\n",
        "\n",
        "This pattern of surprise holds for areas B and C as well. In area B, there are more blue chickens than orange, so we would not be very surprised if we pick up a blue chicken. In area C, where there is an equal number of orange and blue chickens, we would be equally surprised regardless of what color chicken we pick up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculating Surprise\n",
        "\n",
        "Surprise is calculated as the log of the inverse of the probability. This is because when the probability of an event is 1 (i.e., the event always occurs), we want the surprise to be 0. However, if we simply took the inverse of the probability, we would get 1 instead of 0. Therefore, we take the log of the inverse of the probability.\n",
        "\n",
        "Mathematically, the surprise `S` of an event with probability `p` is given by:\n",
        "\n",
        "$$ S = \\log(\\frac{1}{p}) $$\n",
        "\n",
        "This equation gives us a nice curve where the closer the probability gets to zero, the more surprise we get. But now the curve says there is no surprise when the probability is one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entropy\n",
        "\n",
        "Now that we understand what surprise is, we can discuss entropy. Entropy can be thought of as the average surprise of an event. For example, if we flip a coin that lands on heads 90% of the time and tails 10% of the time, we can calculate the average surprise (or entropy) of this event.\n",
        "\n",
        "Let's denote the surprise of getting heads as `S_Heads` and the surprise of getting tails as `S_Tails`. Then the entropy `E` of the coin is given by:\n",
        "\n",
        "$$ E = p(\\text{Heads}) \\cdot S_{\\text{Heads}} + p(\\text{Tails}) \\cdot S_{\\text{Tails}} $$\n",
        "\n",
        "This equation tells us that the entropy is a weighted average of the surprises of each outcome, with the weights being the probabilities of each outcome. The entropy represents the expected surprise of an event. If we flipped this coin a bunch of times, on average, we would expect the surprise to be equal to the entropy every time we flip the coin.\n",
        "\n",
        "Note: When calculating entropy for outcomes with more than two possibilities, we simply add more terms to the equation, each representing a different outcome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4689955935892812\n",
            "1.0\n",
            "0.08079313589591118\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "def calculate_entropy(probabilities):\n",
        "    \"\"\"\n",
        "    Calculate the entropy given a list of probabilities.\n",
        "\n",
        "    :param probabilities: list of probabilities\n",
        "    :return: entropy\n",
        "    \"\"\"\n",
        "    return -sum([p * math.log(p, 2) for p in probabilities if p > 0])\n",
        "\n",
        "# Test the function with some example probabilities\n",
        "print(calculate_entropy([0.9, 0.1]))  # Example with a coin that gives heads 90% of the time\n",
        "print(calculate_entropy([0.5, 0.5]))  # Example with a fair coin\n",
        "print(calculate_entropy([0.99, 0.01]))  # Example with a coin that gives heads 99% of the time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see from the results, the entropy is highest when the probabilities of the outcomes are equal (as in the case of the fair coin), and it is lowest when one outcome is much more probable than the other (as in the case of the coin that gives heads 99% of the time)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Entropy is a fundamental concept in information theory and data science. It quantifies the amount of uncertainty or surprise associated with a random variable. In the context of machine learning, entropy is often used as a criterion for splitting decision trees because it can effectively measure the impurity of an input set. Understanding entropy can therefore help us build more effective models and gain deeper insights from our data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
