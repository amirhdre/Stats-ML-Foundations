{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lasso (L1) Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we're going to focus on Lasso Regression which is a type of regularized linear regression that uses the L1 regularization. Lasso Regression not only helps in reducing over-fitting but it can help us in feature selection.\n",
        "\n",
        "Just like Ridge Regression, the cost function is altered by adding a penalty equivalent to the absolute value of the magnitude of the coefficients.\n",
        "\n",
        "Let's recall the cost function for Linear Regression:\n",
        "\n",
        "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 $$\n",
        "\n",
        "The cost function for Lasso Regression looks like this:\n",
        "\n",
        "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} |\\theta_j| $$\n",
        "\n",
        "The only difference is the term $\\lambda \\sum_{j=1}^{n} |\\theta_j|$. This term is the sum of the absolute value of all the coefficients in the model. The Lasso Regression will try to minimize it.\n",
        "\n",
        "The hyperparameter $\\lambda$ will decide the amount of penalty that will be added:\n",
        "\n",
        "- $\\lambda$ = 0: The objective becomes same as simple linear regression.\n",
        "- $\\lambda$ = ∞: The coefficients will be zero because of infinite penalty.\n",
        "- 0 < $\\lambda$ < ∞: The magnitude of $\\lambda$ will decide the weightage given to different parts of cost function.\n",
        "\n",
        "The main advantage of Lasso Regression is that it can end up using only a subset of the most important features. In other words, Lasso Regression automatically performs feature selection and outputs a model that is easier to interpret."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start by importing the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create a synthetic regression dataset using `make_regression` function. This function will create a dataset with 100 samples and 30 features. Out of 30 features, 10 are informative and rest are not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X, y, coef = make_regression(n_samples=100, n_features=30, n_informative=10, noise=0.1, coef=True, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's split the dataset into training and testing sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's train our Lasso Regression model. We will start with $\\lambda$ = 0.1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's print out the coefficients of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lasso.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that many coefficients are 0. This means Lasso Regression has performed feature selection and these features have been excluded from the model.\n",
        "\n",
        "Let's now calculate the model score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lasso.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the score of our model is quite high. Thus, despite excluding some of the features, Lasso Regression has managed to perform well on this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In summary, Lasso Regression not only helps in reducing over-fitting but it can help us in feature selection. It is useful when we have a large number of features and we need a simple model."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
