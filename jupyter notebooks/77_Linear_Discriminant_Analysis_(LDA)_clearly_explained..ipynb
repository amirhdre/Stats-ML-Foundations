{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "Linear Discriminant Analysis (LDA) is a well-established machine learning technique for predicting categories. Its main advantages, compared to other classification algorithms such as neural networks and random forests, are that the model is interpretable and that prediction is easy. LDA is also well-featured in popular statistical and machine learning software. This notebook will demonstrate an implementation of LDA with python.\n",
    "\n",
    "## What is Linear Discriminant Analysis (LDA)?\n",
    "\n",
    "LDA is a way of comparing groups for statistical significance. LDA makes some simplifying assumptions about your data:\n",
    "\n",
    "- Each of your groups has multivariate normal (also called multivariate Gaussian) distribution. This is a generalization of the one-dimensional normal distribution to higher dimensions.\n",
    "- Each group has the same covariance matrix. This is a generalization of variance to multiple dimensions.\n",
    "\n",
    "Note that in one dimension, LDA reduces to the Student's t-test.\n",
    "\n",
    "## How does LDA work?\n",
    "\n",
    "LDA works by projecting the feature space (independent variables) onto a smaller subspace while preserving the class-discriminatory information. In other words, it tries to find a new set of axes for the feature space, such that:\n",
    "\n",
    "- Maximizes the distance between the means of the classes.\n",
    "- Minimizes the variation (scatter), within each category.\n",
    "\n",
    "The two criteria can be combined into a single formula that forms a ratio of the between-class variance to the within-class variance. The axes which maximize this ratio are chosen to be the axes of the new feature space.\n",
    "\n",
    "Mathematically, the new axes are the eigenvectors of the following equation:\n",
    "\n",
    "$$S_W^{-1}S_Bv = \\lambda v$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $S_W$ is the within-class scatter matrix.\n",
    "- $S_B$ is the between-class scatter matrix.\n",
    "- $v$ is an eigenvector.\n",
    "- $\\lambda$ is the corresponding eigenvalue.\n",
    "\n",
    "The scatter matrices are defined as follows:\n",
    "\n",
    "$$S_W = \\sum_{i=1}^{c} S_i$$\n",
    "\n",
    "$$S_i = \\sum_{x \\in D_i} (x - m_i) (x - m_i)^T$$\n",
    "\n",
    "$$S_B = \\sum_{i=1}^{c} N_i (m_i - m) (m_i - m)^T$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $c$ is the number of classes.\n",
    "- $D_i$ is the set of observations of the $i$-th class.\n",
    "- $x$ is a single observation.\n",
    "- $m_i$ is the mean of the observations of the $i$-th class.\n",
    "- $m$ is the overall mean of the observations, regardless of class.\n",
    "- $N_i$ is the number of observations of the $i$-th class.\n",
    "\n",
    "The eigenvectors are ranked by their corresponding eigenvalues. The eigenvector with the highest eigenvalue is the most informative axis, the eigenvector with the second highest eigenvalue is the second most informative axis, and so on. The set of eigenvectors forms a new set of axes that can be used to represent the observations.\n",
    "\n",
    "## What are the assumptions of LDA?\n",
    "\n",
    "LDA makes several assumptions:\n",
    "\n",
    "- **Independence**: The independent variables are statistically independent.\n",
    "- **Normality**: The independent variables follow a multivariate normal distribution.\n",
    "- **Homogeneity of Variances**: The independent variables have the same variance.\n",
    "\n",
    "If these assumptions are violated, then LDA may not be the best method for the data.\n",
    "\n",
    "## What are the applications of LDA?\n",
    "\n",
    "LDA has been successfully applied to many real-world problems:\n",
    "\n",
    "- **Face Recognition**: LDA is used to recognize faces by projecting the faces into a lower-dimensional space and measuring the distances between faces.\n",
    "- **Marketing**: LDA can be used to segment the market and target marketing campaigns to specific customer groups.\n",
    "- **Medical Diagnosis**: LDA can be used to classify patients into different disease categories.\n",
    "\n",
    "## What are the limitations of LDA?\n",
    "\n",
    "LDA has several limitations:\n",
    "\n",
    "- **Sensitive to Outliers**: LDA is sensitive to outliers because they can significantly affect the means and the variances of the independent variables.\n",
    "- **Assumptions**: The assumptions of LDA may not be valid for all data sets.\n",
    "- **Multicollinearity**: If the independent variables are highly correlated, then the performance of LDA may be degraded.\n",
    "- **High-Dimensional Data**: When the number of independent variables is large, the calculation of the scatter matrices and their eigenvalues can be computationally expensive.\n",
    "\n",
    "## How does LDA compare to other methods?\n",
    "\n",
    "LDA is similar to Principal Component Analysis (PCA), another popular dimensionality reduction technique. Both LDA and PCA project the feature space onto a lower-dimensional space. PCA tries to find the axes with maximum variance, while LDA tries to find the axes for best class separability. LDA can only be used for classification problems, while PCA can be used for any type of data.\n",
    "\n",
    "In the next section, we will apply LDA on a real-world data set using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Create an LDA object\n",
    "lda = LDA()\n",
    "\n",
    "# Fit the model\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = lda.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Results\n",
    "\n",
    "LDA provides the coefficients that gives the linear combinations of the features that best separate the classes. The larger the coefficient, the more important the corresponding feature is in separating the classes. Let's examine the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = lda.coef_\n",
    "print('Coefficients:\\n', coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients tell us how much the corresponding feature contributes to the separation of the classes. The larger the coefficient, the more important the corresponding feature is in separating the classes.\n",
    "\n",
    "## Visualizing the Results\n",
    "\n",
    "To visualize the separation of the classes, we can project the data onto the new axes and plot the projected data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the data onto the new axes\n",
    "X_lda = lda.transform(X)\n",
    "\n",
    "# Plot the projected data\n",
    "plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y)\n",
    "plt.xlabel('LD1')\n",
    "plt.ylabel('LD2')\n",
    "plt.title('LDA Projection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, different colors represent different classes. We can see that the classes are well separated, indicating that LDA can effectively classify the data.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we have introduced Linear Discriminant Analysis (LDA) and demonstrated how to perform LDA in Python using the sklearn library. We have also shown how to interpret the results of LDA and visualized the separation of classes. LDA is a powerful tool for classification and dimensionality reduction that can be used in a wide variety of fields."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
