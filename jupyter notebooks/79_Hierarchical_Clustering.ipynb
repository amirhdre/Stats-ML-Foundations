{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering is a type of unsupervised machine learning algorithm used to cluster unlabeled data points. Like many other clustering algorithms, hierarchical clustering does not require a pre-specified number of clusters. However, in practice, the hierarchical clustering is often interpreted via dendrogram, which requires user specification of the number of clusters.\n",
    "\n",
    "A big advantage of hierarchical clustering is that it results in an attractive tree-based representation of the observations, called a dendrogram. Hierarchical clustering, which is deterministic, offers some advantages over K-means, which is stochastic. In particular, all observations are part of the dendrogram, which gives an additional representation of the observations. \n",
    "\n",
    "There are two types of hierarchical clustering, Divisive and Agglomerative.\n",
    "\n",
    "1. **Agglomerative**: This is a \"bottom-up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n",
    "\n",
    "2. **Divisive**: This is a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.\n",
    "\n",
    "In this notebook, we will be looking at Agglomerative clustering which is the most common type of hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering\n",
    "\n",
    "Agglomerative clustering works in a bottom-up manner. It starts by considering each data point as a separate cluster. It then iteratively merges the closest pair of clusters and repeats this step until only a single cluster is left.\n",
    "\n",
    "The steps can be summarized as follows:\n",
    "\n",
    "1. Make each data point a single-point cluster → that forms $N$ clusters\n",
    "2. Take the two closest data points and make them one cluster → that forms $N-1$ clusters\n",
    "3. Take the two closest clusters and make them one cluster → Repeat this step until there is only one cluster\n",
    "\n",
    "The results of hierarchical clustering can be shown using a dendrogram. The dendrogram can be interpreted as:\n",
    "\n",
    "- The x-axis contains the samples.\n",
    "- The y-axis represents the distance between these samples.\n",
    "- The vertical line with maximum distance is the blue line and hence we can decide a threshold of 6 and cut the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Sample Dataset\n",
    "\n",
    "Let's create a sample dataset with make_blobs function which is used to generate sample points around c centers (randomly chosen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Sample Dataset\n",
    "data, labels = make_blobs(n_samples=500, centers=6, random_state=0, cluster_std=0.70)\n",
    "\n",
    "# Visualizing the Data\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(data[:,0], data[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering on the Data\n",
    "\n",
    "Let's apply the hierarchical clustering method to this data and see how it performs. We will start by plotting the dendrogram for our data points. We will use the `scipy` library for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dendrogram\n",
    "plt.figure(figsize=(10, 7))  \n",
    "plt.title(\"Dendrogram\")  \n",
    "dendrogram = shc.dendrogram(shc.linkage(data, method='ward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above Dendrogram, it can be observed that the optimal number of clusters should be 6 as that is where the biggest distance we can vertically without crossing any horizontal line (Euclidean distance).\n",
    "\n",
    "Now, let's train the hierarchical clustering model on our data and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "cluster = AgglomerativeClustering(n_clusters=6, affinity='euclidean', linkage='ward')  \n",
    "predicted_labels = cluster.fit_predict(data)\n",
    "\n",
    "# Visualizing the Clusters\n",
    "plt.figure(figsize=(10, 7))  \n",
    "plt.scatter(data[:,0], data[:,1], c=cluster.labels_, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plot, we can observe that the points have been clustered correctly in their respective clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right Number of Clusters\n",
    "\n",
    "Often, the data you'll be working with will have multiple dimensions making it difficult to visualize. As a result, there won't necessarily be a way to check the output of the clustering algorithm as we have in the 2D example above. This means that it becomes important to have a way to measure the \"goodness\" of the clustering to help determine the correct number of clusters. One such method is the **Elbow method**.\n",
    "\n",
    "The Elbow method uses the sum of squared distance (SSE) between data points and their assigned clusters' centroids. We pick k at the spot where SSE starts to flatten out and forming an elbow. The Python implementation for the same is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# k means determine k\n",
    "distortions = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(data)\n",
    "    kmeanModel.fit(data)\n",
    "    distortions.append(sum(np.min(cdist(data, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / data.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the elbow point is at k=2. This is an indication that k=2 should be a good choice for this data set.\n",
    "\n",
    "Note: The Elbow method is a \"rule-of-thumb\" approach to finding the optimal number of clusters. As such, the 'elbow' cannot always be unambiguously identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have understood the concept of hierarchical clustering and how it can be used to cluster data points into groups. We have also seen how to use Python and Scikit Learn to implement hierarchical clustering and visualize the clusters. Lastly, we looked at how to determine the correct number of clusters for a given dataset using the Elbow Method. Hierarchical clustering is a powerful technique for data analysis and can be used in a wide variety of fields, including machine learning, data mining, and bioinformatics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
