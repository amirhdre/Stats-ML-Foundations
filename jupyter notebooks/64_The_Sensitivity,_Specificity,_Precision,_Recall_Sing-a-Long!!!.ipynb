{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sensitivity, Specificity, Precision, Recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we'll introduce the concepts of Sensitivity, Specificity, Precision and Recall. These are key metrics used to evaluate the performance of a classification model in Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sensitivity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sensitivity, also known as True Positive Rate (TPR), measures the proportion of actual positive cases which are correctly identified. In other words, it's the ability of a model to find all the relevant cases within a dataset. The formula to calculate sensitivity is:\n",
        "\n",
        "$$ Sensitivity = \\frac{True Positives}{True Positives + False Negatives} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specificity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Specificity, also known as True Negative Rate (TNR), measures the proportion of actual negative cases which are correctly identified. It's the ability of the model to find all the negative cases. The formula to calculate specificity is:\n",
        "\n",
        "$$ Specificity = \\frac{True Negatives}{True Negatives + False Positives} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Precision, also known as Positive Predictive Value (PPV), measures the proportion of actual positive cases out of the predicted positive cases. It's the ability of the classifier not to label a negative sample as positive. The formula to calculate precision is:\n",
        "\n",
        "$$ Precision = \\frac{True Positives}{True Positives + False Positives} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall, also known as Sensitivity, Hit Rate, or True Positive Rate (TPR), measures the proportion of actual positive cases which are correctly identified. The formula to calculate recall is the same as the one for sensitivity as they refer to the same concept:\n",
        "\n",
        "$$ Recall = Sensitivity = \\frac{True Positives}{True Positives + False Negatives} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's take a look at these concepts in a practical way by applying them to a binary classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sensitivity: 0.85\n",
            "Specificity: 0.89\n",
            "Precision: 0.86\n",
            "Recall: 0.85\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create a simple binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a logistic regression model\n",
        "clf = LogisticRegression(random_state=42).fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "# Calculate sensitivity, specificity, precision, and recall\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f'Sensitivity: {sensitivity:.2f}')\n",
        "print(f'Specificity: {specificity:.2f}')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "papermill": {
      "duration": 1.906,
      "end_time": "2020-07-14T00:42:55.317Z",
      "environment_variables": {},
      "exception": null,
      "input_path": "input.ipynb",
      "output_path": "output.ipynb",
      "parameters": {},
      "start_time": "2020-07-14T00:42:53.411Z",
      "version": "2.1.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
