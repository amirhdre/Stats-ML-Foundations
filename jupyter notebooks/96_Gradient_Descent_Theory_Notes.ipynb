{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "574d5181",
   "metadata": {},
   "source": [
    "\n",
    "# Gradient Descent - Theory Notes\n",
    "\n",
    "## What is Gradient Descent?\n",
    "Gradient Descent is an optimization algorithm used to minimize a loss function, $L(\\theta)$. It iteratively updates model parameters in the direction of the steepest descent (negative gradient) of the loss function.\n",
    "\n",
    "The update rule is:\n",
    "$$\n",
    "\\theta_{new} = \\theta_{old} - \\alpha \\frac{\\partial L}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "- $\\alpha$: Learning rate (step size).\n",
    "- $\\frac{\\partial L}{\\partial \\theta}$: Gradient of the loss function.\n",
    "\n",
    "## Steps in Gradient Descent\n",
    "1. **Define the Loss Function**: Quantifies the error. For linear regression, use Sum of Squared Residuals (SSR):\n",
    "   $$\n",
    "   L(m, b) = \\sum_{i=1}^n (y_i - (m x_i + b))^2\n",
    "   $$\n",
    "\n",
    "2. **Compute the Gradients**:\n",
    "   - For the intercept ($b$):\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial b} = -2 \\sum_{i=1}^n (y_i - (m x_i + b))\n",
    "     $$\n",
    "   - For the slope ($m$):\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial m} = -2 \\sum_{i=1}^n x_i (y_i - (m x_i + b))\n",
    "     $$\n",
    "\n",
    "3. **Update Parameters**:\n",
    "   $$\n",
    "   b_{new} = b_{old} - \\alpha \\frac{\\partial L}{\\partial b}\n",
    "   $$\n",
    "   $$\n",
    "   m_{new} = m_{old} - \\alpha \\frac{\\partial L}{\\partial m}\n",
    "   $$\n",
    "\n",
    "4. **Iterate Until Convergence**: Repeat updates until gradients are small or a maximum number of iterations is reached.\n",
    "\n",
    "## Example: Single Parameter Optimization\n",
    "Given $L(w) = (w - 3)^2$, the derivative is:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = 2(w - 3)\n",
    "$$\n",
    "The update rule becomes:\n",
    "$$\n",
    "w_{new} = w_{old} - \\alpha \\cdot 2(w - 3)\n",
    "$$\n",
    "\n",
    "## Key Notes\n",
    "- **Learning Rate** ($\\alpha$): Controls step size. Too large: Overshooting. Too small: Slow convergence.\n",
    "- **Convergence Criteria**: Stop when parameter updates are negligible (step size $< \\epsilon$).\n",
    "\n",
    "Gradient Descent variants:\n",
    "- **Batch Gradient Descent**: Uses the entire dataset.\n",
    "- **Stochastic Gradient Descent (SGD)**: Uses one data point per step for faster updates.\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}