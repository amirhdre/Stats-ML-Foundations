{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6e6c5d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Backpropagation in Neural Networks: Main Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5baf442",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "Backpropagation is a key algorithm for training neural networks by optimizing parameters such as weights and biases.\n",
    "This notebook explains the main ideas of backpropagation using the chain rule and gradient descent. The concepts\n",
    "are broken down step-by-step, with accompanying examples and mathematical notations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b5ddfd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before diving into backpropagation, ensure you are familiar with:\n",
    "1. Neural Networks\n",
    "2. The Chain Rule in calculus\n",
    "3. Gradient Descent\n",
    "\n",
    "If not, review the necessary resources or tutorials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb75b9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Simple Example of a Neural Network\n",
    "\n",
    "Consider a neural network with a single hidden layer designed to predict whether drug dosages (low, medium, high) are effective.\n",
    "\n",
    "### Neural Network Workflow\n",
    "1. Input features (e.g., dosage levels) pass through the network.\n",
    "2. Activation functions, weights, and biases adjust the input to produce an output prediction.\n",
    "3. Backpropagation calculates how weights and biases should be updated to minimize prediction error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67c4735",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Backpropagation Process\n",
    "\n",
    "The goal of backpropagation is to minimize the error between the predicted values and the actual values in the dataset.\n",
    "\n",
    "### Steps:\n",
    "1. **Forward Pass:** Calculate predictions using the current parameters.\n",
    "2. **Error Calculation:** Compute the loss (e.g., sum of squared residuals).\n",
    "3. **Backward Pass:** Use the chain rule to calculate the gradient of the loss function with respect to each parameter.\n",
    "4. **Parameter Update:** Adjust weights and biases using gradient descent.\n",
    "\n",
    "Mathematically, the process involves:\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "where:\n",
    "- $L$: Loss function (sum of squared residuals)\n",
    "- $y_i$: Observed values\n",
    "- $\\hat{y}_i$: Predicted values\n",
    "- $n$: Number of observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8705d81",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Using the Chain Rule\n",
    "\n",
    "To compute gradients, we apply the chain rule. For a parameter $b_3$, the gradient is:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_3} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial b_3}\n",
    "$$\n",
    "\n",
    "### Example:\n",
    "Given a loss function:\n",
    "$$\n",
    "L = \\sum_{i=1}^{3} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "1. Compute $\\frac{\\partial L}{\\partial \\hat{y}}$:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}} = -2 (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "2. Compute $\\frac{\\partial \\hat{y}}{\\partial b_3}$:\n",
    "For a simple network, $\\hat{y} = f(x; b_3)$, where $b_3$ is added directly, so:\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}}{\\partial b_3} = 1\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_3} = -2 (y_i - \\hat{y}_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9823447",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Gradient descent updates parameters iteratively to minimize the loss function.\n",
    "\n",
    "The update rule for a parameter $b_3$:\n",
    "$$\n",
    "b_3 \\leftarrow b_3 - \\alpha \\frac{\\partial L}{\\partial b_3}\n",
    "$$\n",
    "where:\n",
    "- $\\alpha$: Learning rate (step size)\n",
    "- $\\frac{\\partial L}{\\partial b_3}$: Gradient\n",
    "\n",
    "### Example Update:\n",
    "Given $\\frac{\\partial L}{\\partial b_3} = -15.7$ and $\\alpha = 0.1$:\n",
    "$$\n",
    "b_3 \\leftarrow 0 - (0.1)(-15.7) = 1.57\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d81247",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Iterative Optimization\n",
    "\n",
    "Backpropagation uses the gradient descent process repeatedly:\n",
    "1. Compute gradients for all parameters (weights and biases).\n",
    "2. Update parameters using the gradient descent rule.\n",
    "3. Repeat until convergence (i.e., minimal loss).\n",
    "\n",
    "In practice, this is implemented efficiently for large networks using frameworks such as TensorFlow or PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766eaa6c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Backpropagation combines the chain rule and gradient descent to optimize neural network parameters. By iteratively\n",
    "minimizing the loss function, it ensures that the model learns to make accurate predictions.\n",
    "\n",
    "Understanding these concepts is crucial for designing and training deep learning models.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}