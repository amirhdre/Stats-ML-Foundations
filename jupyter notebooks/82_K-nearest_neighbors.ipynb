{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN)\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a simple, easy-to-understand, versatile, and one of the topmost machine learning algorithms. KNN algorithm used for both classification and regression problems. The KNN algorithm is based on feature similarity, meaning how closely out-of-sample features resemble our training set determines how we classify a given data point.\n",
    "\n",
    "![KNN](https://miro.medium.com/max/753/1*2zYNhLc522h0zftD1zDh2g.png)\n",
    "\n",
    "Image source: [Medium](https://medium.com/@adi.bronshtein/a-quick-introduction-to-k-nearest-neighbors-algorithm-62214cea29c7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does KNN work?\n",
    "\n",
    "Given a new observation, KNN algorithm goes through the whole dataset to find the k-observations that are nearest to the measurement of the new observation. These k observations are called the k-nearest neighbors.\n",
    "\n",
    "If KNN is used for classification, the output can be calculated as the class with the highest frequency from the K-most similar instances. Each instance in essence votes for their class and the class with the most votes is taken as the prediction.\n",
    "\n",
    "If KNN is used for regression, the output can be calculated as the average of the numerical target of the K-most similar instances.\n",
    "\n",
    "The distance between two points can be Euclidean, Manhattan, Minkowski and Hamming distance. First three functions are used for continuous function and fourth one (Hamming) for categorical variables. If we are dealing with the large number of features, it's suggested to use standardization before applying KNN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Algorithm\n",
    "\n",
    "1. Load the data\n",
    "2. Initialize K to your chosen number of neighbors\n",
    "3. For each example in the data\n",
    "  1. Calculate the distance between the query example and the current example from the data.\n",
    "  2. Add the distance and the index of the example to an ordered collection\n",
    "4. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances\n",
    "5. Pick the first K entries from the sorted collection\n",
    "6. Get the labels of the selected K entries\n",
    "7. If regression, return the mean of the K labels\n",
    "8. If classification, return the mode of the K labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "dataset = pd.read_csv('diabetes.csv')\n",
    "print(len(dataset))\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X = dataset.iloc[:, 0:8]\n",
    "y = dataset.iloc[:, 8]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model: Init K-NN\n",
    "classifier = KNeighborsClassifier(n_neighbors=11, p=2, metric='euclidean')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we used the KNN algorithm to build a model on the Pima Indians diabetes dataset. We used 'euclidean' distance function and the number of neighbors was 11. The accuracy was then calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "1. Aha, D.W., Kibler, D., & Albert, M.K. (1991). Instance-based learning algorithms. Machine Learning, 6, 37-66.\n",
    "2. Altman, N. S. (1992). An introduction to kernel and nearest-neighbor nonparametric regression. The American Statistician, 46(3), 175-185.\n",
    "3. Cover, T. and Hart, P. (1967). Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13, 21-27.\n",
    "4. Dasarathy, B. (1991). Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. ISBN 0-8186-8930-7.\n",
    "5. Gates, G. W. (1972). The reduced nearest neighbor rule. IEEE Transactions on Information Theory, May 1972, 431-433.\n",
    "6. Hart, P. (1968). The condensed nearest neighbor rule. IEEE Transactions on Information Theory, May 1968, 515-516.\n",
    "7. Shortliffe, E. H. (1975). Computer-Based Medical Consultations: MYCIN. Elsevier/North Holland.\n",
    "8. Skalak, D. B. (1993). Prototype and feature selection by sampling and random mutation hill climbing algorithms. Proceedings of the Eleventh International Conference on Machine Learning, 293-301.\n",
    "9. Zhang, H. (2006). The optimality of Naive Bayes. Proceedings of the Seventeenth International Florida Artificial Intelligence Research Society Conference, 562-567."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}